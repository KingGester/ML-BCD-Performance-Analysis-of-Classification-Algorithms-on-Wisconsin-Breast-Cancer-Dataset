# ML-BCD-Performance-Analysis-of-Classification-Algorithms-on-Wisconsin-Breast-Cancer-Dataset

ğŸ“Œ Scroll down for the English version â¬‡ï¸
# Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ø·Ø§Ù† Ø³ÛŒÙ†Ù‡

## ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø±Ø·Ø§Ù† Ø³ÛŒÙ†Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Breast Cancer Wisconsin Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ø¯. Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ØŒ ØªØ¹ÛŒÛŒÙ† Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØªØ§ÛŒÛŒ (Ø®ÙˆØ´â€ŒØ®ÛŒÙ… ÛŒØ§ Ø¨Ø¯Ø®ÛŒÙ…) Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.

## Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
- Gaussian Naive Bayes
- K-Nearest Neighbors (KNN)
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- Logistic Regression
- Gradient Boosting

## ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
- Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MinMaxScaler
- Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹ (K-fold Cross-Validation)
- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¯Ù‚ØªØŒ ØµØ­ØªØŒ Ùˆ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ
- ØªØ¬Ø³Ù…â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„â€ŒÙ‡Ø§
- ØªØ­Ù„ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ø§ØºØªØ´Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø±ØªØ±
- Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø®ØªÛŒ
- Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC
- ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§

## Ù†ØªØ§ÛŒØ¬ Ú©Ù„ÛŒØ¯ÛŒ
- SVM Ø¨Ø§ Ø¯Ù‚Øª** (98.24%)** Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø± Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- Logistic Regression Ùˆ KNN Ù‡Ø± Ø¯Ùˆ Ø¨Ø§ Ø¯Ù‚Øª**( 95.82%) **Ø¯Ø± Ø±ØªØ¨Ù‡ Ø¯ÙˆÙ… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯
- Random Forest Ø¨Ø§ Ø¯Ù‚Øª**( 95.61% )**Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
- Decision Tree Ø¨Ø§ Ø¯Ù‚Øª**( 90.99% )**Ø¶Ø¹ÛŒÙâ€ŒØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø± Ù…ÛŒØ§Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø§Ø±Ø¯
- Ù…Ø§ØªØ±ÛŒØ³ Ø§ØºØªØ´Ø§Ø´ Random Forest Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªÙ†Ù‡Ø§ 5 Ù…ÙˆØ±Ø¯ Ø§Ø² 114 Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
- ØªØ­Ù„ÛŒÙ„ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù‡Ø³ØªÙ‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¯Ø§Ø±Ù†Ø¯

## Ù†Ø­ÙˆÙ‡ Ø§Ø¬Ø±Ø§
1. Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²:
```bash
pip install -r requirements.txt
```
2. Ø§Ø¬Ø±Ø§ÛŒ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Jupyter:
```bash
jupyter notebook project1.ipynb
```
3. Ø§Ø¬Ø±Ø§ÛŒ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ:
```bash
jupyter notebook Additional_Analysis.ipynb
```
4. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§:
```bash
python model_comparison.py
```
5. Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
```bash
python feature_analysis.py
```

## Ù†ØªØ§ÛŒØ¬ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…ØªÙ‚Ø§Ø·Ø¹ (K=5)
- Support Vector Machine: Ø¯Ù‚Øª 98.24% (Â±0.88%)
- Logistic Regression: Ø¯Ù‚Øª 95.82% (Â±1.62%)
- K-Nearest Neighbors: Ø¯Ù‚Øª 95.82% (Â±2.13%)
- Gradient Boosting: Ø¯Ù‚Øª 95.38% (Â±1.28%)
- Random Forest: Ø¯Ù‚Øª 95.16% (Â±1.32%)
- Naive Bayes: Ø¯Ù‚Øª 93.41% (Â±2.09%)
- Decision Tree: Ø¯Ù‚Øª 90.99% (Â±2.13%)

## ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
- Python 3.13.2
- scikit-learn
- pandas
- numpy
- matplotlib
- seaborn
- jupyter
- StandardScaler Ùˆ MinMaxScaler Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

## Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡
```
ML_project/
â”œâ”€â”€ project1.ipynb             # Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø§ØµÙ„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
â”œâ”€â”€ model_comparison.py        # Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§
â”œâ”€â”€ feature_analysis.py        # Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
â”œâ”€â”€ Additional_Analysis.ipynb  # Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
â”œâ”€â”€ Model_Analysis.md          # Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
â”œâ”€â”€ README.md                  # Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
â””â”€â”€ requirements.txt           # ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
```
# ğŸ”¬ Comparison of Machine Learning Algorithms for Breast Cancer Detection

## Project Description
This project compares the performance of various machine learning algorithms in predicting breast cancer using the standard Breast Cancer Wisconsin dataset. The main goal is to determine the most suitable algorithm for binary classification (benign vs. malignant).

## Algorithms Used
- Gaussian Naive Bayes  
- K-Nearest Neighbors (KNN)  
- Decision Tree  
- Random Forest  
- Support Vector Machine (SVM)  
- Logistic Regression  
- Gradient Boosting  

## Project Features
- Data preprocessing using MinMaxScaler  
- K-fold cross-validation  
- Evaluation with metrics: accuracy, precision, recall  
- Comparative visualization of model results  
- Confusion matrix analysis for top-performing models  
- Feature importance analysis in tree-based models  
- ROC curve comparisons  
- Feature correlation analysis  

## Key Results
- **SVM** achieved the highest cross-validation accuracy: **98.24%**
- **Logistic Regression** and **KNN** both scored **95.82%**
- **Random Forest** performed best on the test set: **95.61%**
- **Decision Tree** had the weakest performance: **90.99%**
- Confusion matrix of Random Forest shows only 5 misclassified samples out of 114  
- Feature importance analysis shows features related to **cell uniformity** and **nucleus size** were most impactful

## How to Run
1. Install the required packages:
```bash
pip install -r requirements.txt
```
2. Run the main Jupyter Notebook:
```bash
jupyter notebook project1.ipynb
```
3. Run additional analysis:
```bash
jupyter notebook Additional_Analysis.ipynb
```
4.Run model comparison script:
```bash
python model_comparison.py
```
5. Run feature analysis script::
```bash
python feature_analysis.py
```
Cross-Validation Results (K=5)
Support Vector Machine: 98.24% (Â±0.88%)

Logistic Regression: 95.82% (Â±1.62%)

K-Nearest Neighbors: 95.82% (Â±2.13%)

Gradient Boosting: 95.38% (Â±1.28%)

Random Forest: 95.16% (Â±1.32%)

Naive Bayes: 93.41% (Â±2.09%)

Decision Tree: 90.99% (Â±2.13%)

Technologies Used
Python 3.13.2

scikit-learn

pandas

numpy

matplotlib

seaborn

jupyter

StandardScaler & MinMaxScaler for normalization

Project Structure
```
ML_project/
â”œâ”€â”€ project1.ipynb             # Main project notebook
â”œâ”€â”€ model_comparison.py        # Model comparison script
â”œâ”€â”€ feature_analysis.py        # Feature analysis script
â”œâ”€â”€ Additional_Analysis.ipynb  # Additional analysis notebook
â”œâ”€â”€ Model_Analysis.md          # Analytical report
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ requirements.txt           # Project dependencies
```



